#!/usr/bin/env python3
"""
Nova CI-Rescue CLI interface.
"""

import typer
import os
import subprocess
import time
from pathlib import Path
from typing import Optional
from datetime import datetime
from nova.tools.datetime_utils import now_utc, delta_between, seconds_between
from rich.console import Console
from rich.table import Table

from nova.runner import TestRunner
from nova.agent import AgentState
from nova.telemetry.logger import JSONLLogger
from nova.config import get_settings
from nova.tools.git import GitBranchManager
from nova import __version__

def version_callback(value: bool):
    if value:
        console = Console()
        console.print(f"nova {__version__}")
        raise typer.Exit()

app = typer.Typer(
    name="nova",
    help="Nova CI-Rescue: Automated test fixing agent",
    add_completion=False,
)
console = Console()

@app.callback()
def main(
    version: bool = typer.Option(
        None,
        "--version",
        callback=version_callback,
        is_eager=True,
        help="Show version and exit"
    )
):
    """Nova CI-Rescue: Automated test fixing agent"""
    pass


def print_exit_summary(state: AgentState, reason: str, elapsed_seconds: float = None, verbose: bool = False) -> None:
    """
    Print a comprehensive summary when exiting the agent loop.
    
    Args:
        state: The current agent state
        reason: The reason for exit (timeout, max_iters, success, etc.)
        elapsed_seconds: Optional elapsed time in seconds
    """
    console.print("\n" + "=" * 60)
    console.print("[bold]EXECUTION SUMMARY[/bold]")
    console.print("=" * 60)
    
    # Exit reason with appropriate styling
    if reason == "success":
        console.print(f"[bold green]‚úÖ Exit Reason: SUCCESS - All tests passing![/bold green]")
    elif reason == "timeout":
        console.print(f"[bold red]‚è∞ Exit Reason: TIMEOUT - Exceeded {state.timeout_seconds}s limit[/bold red]")
    elif reason == "max_iters":
        console.print(f"[bold red]üîÑ Exit Reason: MAX ITERATIONS - Reached {state.max_iterations} iterations[/bold red]")
    elif reason == "no_patch":
        console.print(f"[bold yellow]‚ö†Ô∏è Exit Reason: NO PATCH - Could not generate fix[/bold yellow]")
    elif reason == "patch_rejected":
        console.print(f"[bold yellow]‚ö†Ô∏è Exit Reason: PATCH REJECTED - Critic rejected patch[/bold yellow]")
    elif reason == "patch_error":
        console.print(f"[bold red]‚ùå Exit Reason: PATCH ERROR - Failed to apply patch[/bold red]")
    elif reason == "interrupted":
        console.print(f"[bold yellow]üõë Exit Reason: INTERRUPTED - User cancelled operation[/bold yellow]")
    elif reason == "error":
        console.print(f"[bold red]‚ùå Exit Reason: ERROR - Unexpected error occurred[/bold red]")
    else:
        console.print(f"[bold yellow]Exit Reason: {reason.upper()}[/bold yellow]")
    
    console.print()
    
    # Statistics
    console.print("[bold]Statistics:[/bold]")
    console.print(f"  ‚Ä¢ Iterations completed: {state.current_iteration}/{state.max_iterations}")
    console.print(f"  ‚Ä¢ Patches applied: {len(state.patches_applied)}")
    console.print(f"  ‚Ä¢ Initial failures: {state.initial_failures}")
    console.print(f"  ‚Ä¢ Remaining failures: {state.total_failures}")
    
    if state.total_failures == 0:
        console.print(f"  ‚Ä¢ [green]All tests fixed successfully![/green]")
    elif state.failing_tests and state.total_failures < len(state.failing_tests):
        fixed = len(state.failing_tests) - state.total_failures
        console.print(f"  ‚Ä¢ Tests fixed: {fixed}/{len(state.failing_tests)}")
    
    if elapsed_seconds is not None:
        minutes, seconds = divmod(int(elapsed_seconds), 60)
        console.print(f"  ‚Ä¢ Time elapsed: {minutes}m {seconds}s")
    elif hasattr(state, 'start_time') and state.start_time:
        # Handle both datetime and float start_time
        if isinstance(state.start_time, float):
            elapsed = time.time() - state.start_time
        else:
            elapsed = seconds_between(now_utc(), state.start_time)
        minutes, seconds = divmod(int(elapsed), 60)
        console.print(f"  ‚Ä¢ Time elapsed: {minutes}m {seconds}s")
    
    # List saved patches if telemetry is enabled
    from nova.config import get_settings
    settings = get_settings()
    if settings.enable_telemetry and hasattr(state, 'telemetry') and state.telemetry:
        try:
            from pathlib import Path
            run_dir = state.telemetry.run_dir
            if run_dir and Path(run_dir).exists():
                patch_dir = Path(run_dir) / "patches"
                if patch_dir.exists():
                    console.print("\n[bold]üìÑ Saved patches:[/bold]")
                    patches = sorted(patch_dir.glob("*.patch"))
                    if patches:
                        for patch_file in patches:
                            console.print(f"  ‚Ä¢ {patch_file.name}")
                        console.print(f"  [dim](Saved in: {patch_dir})[/dim]")
                    else:
                        console.print("  [dim](No patches saved)[/dim]")
        except Exception as e:
            if verbose:
                console.print(f"[dim]Could not list patches: {e}[/dim]")
    
    console.print("=" * 60)
    console.print()


def _post_ci_start_comment(repo_path: Path, verbose: bool = False) -> None:
    """Post a comment to the PR when Nova starts in CI mode."""
    try:
        import subprocess
        import os
        
        # Check if we're in a GitHub Actions environment
        if not os.environ.get("GITHUB_ACTIONS"):
            if verbose:
                console.print("[dim]Not in GitHub Actions environment, skipping PR comment[/dim]")
            return
            
        # Get PR number from GitHub Actions context
        pr_number = os.environ.get("GITHUB_EVENT_NUMBER")
        if not pr_number:
            # Try to get it from the event payload
            event_path = os.environ.get("GITHUB_EVENT_PATH")
            if event_path and os.path.exists(event_path):
                import json
                with open(event_path) as f:
                    event = json.load(f)
                    pr_number = event.get("pull_request", {}).get("number")
        
        if not pr_number:
            if verbose:
                console.print("[dim]No PR number found in GitHub Actions context[/dim]")
            return
            
        # Create the comment
        comment = """ü§ñ **Nova CI-Rescue has started analyzing this PR**

I'm now running tests to identify failures and will automatically generate fixes.

**Status:** üîÑ Running initial test suite...

I'll update this comment with my findings and any fixes I apply.
"""
        
        # Post the comment using gh CLI
        result = subprocess.run(
            ["gh", "pr", "comment", str(pr_number), "--body", comment],
            capture_output=True,
            text=True,
            cwd=repo_path
        )
        
        if result.returncode != 0 and verbose:
            console.print(f"[yellow]Failed to post PR comment: {result.stderr}[/yellow]")
            
    except Exception as e:
        if verbose:
            console.print(f"[yellow]Error posting PR comment: {e}[/yellow]")


def _post_ci_completion_comment(repo_path: Path, state: AgentState, verbose: bool = False) -> None:
    """Post a completion comment to the PR when Nova finishes in CI mode."""
    try:
        import subprocess
        import os
        
        # Check if we're in a GitHub Actions environment
        if not os.environ.get("GITHUB_ACTIONS"):
            return
            
        # Get PR number
        pr_number = os.environ.get("GITHUB_EVENT_NUMBER")
        if not pr_number:
            event_path = os.environ.get("GITHUB_EVENT_PATH")
            if event_path and os.path.exists(event_path):
                import json
                with open(event_path) as f:
                    event = json.load(f)
                    pr_number = event.get("pull_request", {}).get("number")
        
        if not pr_number:
            return
            
        # Create the completion comment
        fixed_count = len(state.initial_failing_tests) - len(state.failing_tests) if state else 0
        total_count = len(state.initial_failing_tests) if state else 0
        
        comment = f"""‚úÖ **Nova CI-Rescue has completed fixing this PR!**

**Results:**
- üß™ Fixed {fixed_count} out of {total_count} failing tests
- üìù Applied {len(state.patches_applied) if state else 0} patches
- ‚è±Ô∏è Completed in {state.current_iteration if state else 0} iterations

**All tests are now passing!** üéâ

The fixes have been committed directly to this PR branch. Please review the changes.
"""
        
        # Post the comment
        subprocess.run(
            ["gh", "pr", "comment", str(pr_number), "--body", comment],
            capture_output=True,
            text=True,
            cwd=repo_path
        )
            
    except Exception as e:
        if verbose:
            console.print(f"[yellow]Error posting completion comment: {e}[/yellow]")


def _generate_ci_diff_summary(state: AgentState, repo_path: Path) -> None:
    """Generate beautiful diff summary for CI mode."""
    import json
    import os
    import re
    
    # Create output directory
    output_dir = Path(".nova") / "ci_output"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    diff_summary = []
    diff_summary.append("<details>")
    diff_summary.append("<summary>Click to see beautiful code transformations ‚ú®</summary>")
    diff_summary.append("")
    
    # Process each applied patch
    for i, patch_content in enumerate(state.patches_applied):
        # Extract function names and changes from patch
        lines = patch_content.split('\n')
        
        # Look for function definitions in the patch
        func_name = None
        for line in lines:
            func_match = re.search(r'^\+def (\w+)\(', line)
            if func_match:
                func_name = func_match.group(1)
                break
        
        if func_name:
            diff_summary.append(f"#### `{func_name}()` - {func_name.replace('_', ' ').title()} Function")
            diff_summary.append("```diff")
            
            # Show the relevant diff lines
            in_function = False
            for line in lines:
                if line.startswith('@@'):
                    continue
                if 'def ' + func_name in line:
                    in_function = True
                if in_function:
                    if line.startswith('+') and not line.startswith('+++'):
                        # Add checkmark for correct fixes
                        if '# ‚úÖ' not in line:
                            line = line.rstrip() + '  # ‚úÖ Fixed'
                    elif line.startswith('-') and not line.startswith('---'):
                        # Add X for bugs
                        if '# ‚ùå' not in line:
                            line = line.rstrip() + '  # ‚ùå Bug'
                    diff_summary.append(line)
                    # Stop after the function ends
                    if in_function and line.strip() and not line.startswith(' ') and not line.startswith('\t') and not line.startswith('+') and not line.startswith('-'):
                        break
            
            diff_summary.append("```")
            diff_summary.append("")
    
    diff_summary.append("</details>")
    
    # Write to file for GitHub Actions to read
    with open(output_dir / "diff_summary.md", "w") as f:
        f.write('\n'.join(diff_summary))
    
    # Also output to console in CI mode
    console.print("\n[bold]CI Mode - Diff Summary Generated[/bold]")
    console.print(f"Saved to: {output_dir / 'diff_summary.md'}")


@app.command()
def fix(
    repo_path: Path = typer.Argument(
        Path("."),
        help="Path to repository to fix",
        exists=True,
        file_okay=False,
        dir_okay=True,
        resolve_path=True,
    ),
    max_iters: Optional[int] = typer.Option(
        None,
        "--max-iters",
        "-i",
        help="Maximum number of fix iterations (default: 5, can be set via NOVA_MAX_ITERS env var)",
        min=1,
        max=20,
    ),
    timeout: Optional[int] = typer.Option(
        None,
        "--timeout",
        "-t",
        help="Overall timeout in seconds (default: 300, can be set via NOVA_RUN_TIMEOUT_SEC env var)",
        min=60,
        max=7200,
    ),
    verbose: bool = typer.Option(
        False,
        "--verbose",
        "-v",
        help="Enable verbose output",
    ),
    auto_pr: bool = typer.Option(
        False,
        "--auto-pr",
        help="Automatically create PR without prompting",
    ),
    no_telemetry: bool = typer.Option(
        False,
        "--no-telemetry",
        help="Disable telemetry collection for this run",
    ),
    whole_file: bool = typer.Option(
        True,
        "--whole-file/--patch-mode",
        "-w/-p",
        help="Replace entire files instead of using patches (default: whole-file mode)",
    ),
    ci_mode: bool = typer.Option(
        False,
        "--ci-mode",
        help="CI mode: commits to current branch, uses patches, generates beautiful diff output",
    ),
    test_after_review: bool = typer.Option(
        False,
        "--test-after-review",
        help="Run tests after patch before critic review (more accurate but slower)",
    ),
    allow_dirty: bool = typer.Option(
        False,
        "--allow-dirty",
        help="Proceed even if the working tree has uncommitted changes (non-interactive)",
    ),
    junit_report: Optional[Path] = typer.Option(
        None,
        "--junit-report",
        help="Save JUnit XML test report to specified path",
    ),
    quiet: bool = typer.Option(
        False,
        "--quiet",
        "-q",
        help="Minimize output (don't show mode, repo path, etc.)",
    ),
    truncated: bool = typer.Option(
        False,
        "--truncated",
        "--truncate",
        help="Truncate verbose LLM request/response content (use this flag to enable truncation)",
    ),
    demo_mode: bool = typer.Option(
        False,
        "--demo-mode",
        help="Use relaxed safety limits for demo repositories (2000 lines, 100k tokens)",
    ),
    ci: Optional[str] = typer.Option(
        None,
        "--ci",
        help="Custom test command to run instead of default pytest (e.g. 'pytest -q')",
    ),
    model: Optional[str] = typer.Option(
        None,
        "--model",
        help="LLM model to use for this run (overrides NOVA_DEFAULT_LLM_MODEL)",
    ),
    reasoning_effort: Optional[str] = typer.Option(
        None,
        "--reasoning-effort",
        help="Reasoning effort hint (e.g. low|medium|high) overrides NOVA_DEFAULT_REASONING_EFFORT",
    ),
    branch_name: Optional[str] = typer.Option(
        None,
        "--branch-name",
        help="Use a specific branch name for fixes instead of auto-generated",
    ),
):
    """
    Fix failing tests in a repository.
    """
    # Apply environment variable defaults if parameters weren't explicitly provided
    # Allow per-run overrides via flags by updating env and resetting settings cache
    from nova.config import get_settings as _gs
    settings = _gs()
    if model or reasoning_effort:
        import os as _os
        from nova.config import _reset_settings_cache as _reset
        if model:
            _os.environ["NOVA_DEFAULT_LLM_MODEL"] = model
        if reasoning_effort:
            _os.environ["NOVA_DEFAULT_REASONING_EFFORT"] = reasoning_effort
        _reset()
        settings = _gs()
    if max_iters is None:
        max_iters = settings.max_iters
    if timeout is None:
        timeout = settings.run_timeout_sec
    
    # CI mode automatically sets certain options for optimal CI/CD integration
    if ci_mode:
        whole_file = False  # Use patches for precise fixes
        allow_dirty = True  # CI environments may have changes
        auto_pr = False     # Don't create new PRs
        test_after_review = True  # Test patches before critic review for accuracy
        if not quiet:
            console.print(f"[green]Nova CI-Rescue[/green] üöÄ [dim](CI Mode)[/dim]")
    elif not quiet:
        console.print(f"[green]Nova CI-Rescue[/green] üöÄ")
        
    if not quiet:
        console.print(f"Repository: {repo_path}")
        console.print(f"Max iterations: {max_iters}")
        console.print(f"Timeout: {timeout}s")
        if ci_mode:
            console.print(f"[dim]CI Mode: Direct commits ‚Ä¢ Patch fixes ‚Ä¢ PR integration[/dim]")
        console.print()
    
    # Initialize branch manager for nova-fix branch
    git_manager = GitBranchManager(repo_path, verbose=verbose)
    branch_name: Optional[str] = None
    success = False
    telemetry = None
    state = None
    
    # Check for concurrent runs
    from nova.tools.lock import nova_lock
    
    try:
        with nova_lock(repo_path, wait=False):
            # Enforce per-repo run frequency cap
            try:
                settings = get_settings()
                nova_dir = Path(repo_path) / ".nova"
                nova_dir.mkdir(exist_ok=True)
                last_run_file = nova_dir / "last_run.txt"
                import time as _time
                now_ts = int(_time.time())
                # Skip frequency cap for demo repos
                if last_run_file.exists() and not demo_mode:
                    try:
                        last_ts = int(last_run_file.read_text().strip() or "0")
                        if now_ts - last_ts < settings.min_repo_run_interval_sec:
                            remaining = settings.min_repo_run_interval_sec - (now_ts - last_ts)
                            if not quiet:
                                console.print(f"[yellow]‚ö†Ô∏è Run frequency cap: please wait {remaining}s before running Nova again on this repo.[/yellow]")
                            raise typer.Exit(1)
                    except Exception:
                        pass
                # Record start of run
                try:
                    last_run_file.write_text(str(now_ts))
                except Exception:
                    pass
            except Exception:
                pass
            # Check for clean working tree before starting
            if not git_manager._check_clean_working_tree():
                if not quiet:
                    console.print("[yellow]‚ö†Ô∏è Warning: You have uncommitted changes in your working tree.[/yellow]")
                if not allow_dirty:
                    from rich.prompt import Confirm
                    if not Confirm.ask("Proceed and potentially lose these changes?"):
                        if not quiet:
                            console.print("[dim]Aborting nova fix due to uncommitted changes.[/dim]")
                        raise typer.Exit(1)
                else:
                    if not quiet:
                        console.print("[dim]Continuing because --allow-dirty was specified.[/dim]")
            
            # Initialize settings and telemetry first
            settings = get_settings()
            # Override telemetry setting if --no-telemetry flag is used
            telemetry_enabled = settings.enable_telemetry and not no_telemetry
            telemetry = JSONLLogger(settings, enabled=telemetry_enabled)
            if telemetry_enabled:
                telemetry.start_run(repo_path)
        
            # Auto-detect demo mode for known demo repositories
            if not demo_mode and repo_path.name in ['calculator-broken-demo', 'demo-calculator-broken']:
                demo_mode = True
                if not quiet:
                    console.print("[dim]Auto-detected demo repository, using relaxed safety limits[/dim]")
            
            # Initialize agent state
            state = AgentState(
                repo_path=repo_path,
                max_iterations=max_iters,
                timeout_seconds=timeout,
                whole_file_mode=whole_file,
                demo_mode=demo_mode,
                ci_mode=ci_mode,
            )
            state.start_time = now_utc()  # Track start time for PR generation
            
            # Post PR comment when starting in CI mode
            if ci_mode and not quiet:
                _post_ci_start_comment(repo_path, verbose)
            
            # Step 1: Run tests to identify failures BEFORE creating branch
            ci_cmd = ci or os.environ.get("NOVA_CI_CMD")
            runner = TestRunner(
                repo_path,
                verbose=verbose,
                custom_cmd=ci_cmd,
                junit_report_path=str(junit_report) if junit_report else None,
            )
            failing_tests, initial_junit_xml = runner.run_tests()
            
            # Save initial test report
            if initial_junit_xml:
                telemetry.save_test_report(0, initial_junit_xml, report_type="junit")
                # Save to user-specified path if provided
                if junit_report:
                    try:
                        junit_report.parent.mkdir(parents=True, exist_ok=True)
                        junit_report.write_text(initial_junit_xml)
                        console.print(f"[dim]JUnit report saved to: {junit_report}[/dim]")
                    except Exception as e:
                        console.print(f"[yellow]Warning: Could not save JUnit report: {e}[/yellow]")
            
            # Store failures in agent state
            state.add_failing_tests(failing_tests)
            
            # Log the test discovery event
            telemetry.log_event("test_discovery", {
                "total_failures": state.total_failures,
                "failing_tests": state.failing_tests,
                "initial_report_saved": initial_junit_xml is not None
            })
            
            # Check if there are any failures (AC: if zero failures ‚Üí exit 0 with message)
            if not failing_tests:
                console.print("[green]‚úÖ No failing tests found! Repository is already green.[/green]")
                state.final_status = "success"
                telemetry.log_event("completion", {"status": "no_failures"})
                telemetry.end_run(success=True)
                success = True
                return
            
            # Only create branch if we have failing tests to fix
            if ci_mode:
                # In CI mode, work on the current branch
                branch_name = git_manager._get_current_branch()
                if not quiet:
                    console.print(f"[dim]CI Mode: Working on current branch: {branch_name}[/dim]")
            else:
                branch_name = git_manager.create_fix_branch(preferred_name=branch_name)
                if not quiet:
                    console.print(f"[dim]Working on branch: {branch_name}[/dim]")
            
            # Set up signal handler for Ctrl+C
            git_manager.setup_signal_handler()
            
            # Display failing tests in a table
            console.print(f"\n[bold red]Found {len(failing_tests)} failing test(s):[/bold red]")
            
            table = Table(title="Failing Tests", show_header=True, header_style="bold magenta")
            table.add_column("Test Name", style="cyan", no_wrap=False)
            table.add_column("Location", style="yellow")
            table.add_column("Error", style="red", no_wrap=False)
            
            for test in failing_tests:
                location = f"{test.file}:{test.line}" if test.line > 0 else test.file
                
                # Extract the most relevant error line (same logic as test runner)
                error_lines = test.short_traceback.split('\n')
                error_preview = "Test failed"
                for line in error_lines:
                    if line.strip().startswith("E"):
                        error_preview = line.strip()[2:].strip()  # Remove "E " prefix
                        break
                    elif "AssertionError" in line or "assert" in line:
                        error_preview = line.strip()
                        break
                
                # Truncate if too long for table display
                if len(error_preview) > 80:
                    error_preview = error_preview[:77] + "..."
                
                table.add_row(test.name, location, error_preview)
            
            console.print(table)
            console.print()
            
            # Prepare planner context (AC: planner prompt contains failing tests table)
            planner_context = state.get_planner_context()
            failures_table = runner.format_failures_table(failing_tests)
            
            if verbose:
                console.print("[dim]Planner context prepared with failing tests:[/dim]")
                console.print(failures_table)
                console.print()
            
            # Set branch info in AgentState for reference
            state.branch_name = branch_name
            state.original_commit = git_manager._get_current_head()
            
            # Import our apply patch node
            from nova.nodes.apply_patch import apply_patch
            
            # Initialize the LLM agent (enhanced version with full Planner/Actor/Critic)
            try:
                from nova.agent.llm_agent_enhanced import EnhancedLLMAgent
                llm_agent = EnhancedLLMAgent(repo_path, verbose=verbose, truncated=truncated)
                
                # Determine which model we're using
                model_name = settings.default_llm_model
                if not quiet:
                    if "gpt" in model_name.lower():
                        reasoning_effort = getattr(settings, 'default_reasoning_effort', 'medium')
                        if "gpt-5" in model_name.lower():
                            console.print(f"[dim]Using OpenAI {model_name} (reasoning effort: {reasoning_effort}) for autonomous test fixing[/dim]")
                        else:
                            console.print(f"[dim]Using OpenAI {model_name} for autonomous test fixing[/dim]")
                    elif "claude" in model_name.lower():
                        console.print(f"[dim]Using Anthropic {model_name} for autonomous test fixing[/dim]")
                    else:
                        console.print(f"[dim]Using {model_name} for autonomous test fixing[/dim]")
                    
            except ImportError as e:
                console.print(f"[yellow]Warning: Could not import enhanced LLM agent: {e}[/yellow]")
                console.print("[yellow]Falling back to basic LLM agent[/yellow]")
                try:
                    from nova.agent.llm_agent import LLMAgent
                    llm_agent = LLMAgent(repo_path)
                except Exception as e2:
                    console.print(f"[yellow]Warning: Could not initialize LLM agent: {e2}[/yellow]")
                    console.print("[yellow]Falling back to mock agent for demo[/yellow]")
                    from nova.agent.mock_llm import MockLLMAgent
                    llm_agent = MockLLMAgent(repo_path)
            except Exception as e:
                console.print(f"[yellow]Warning: Could not initialize enhanced LLM agent: {e}[/yellow]")
                console.print("[yellow]Falling back to mock agent for demo[/yellow]")
                from nova.agent.mock_llm import MockLLMAgent
                llm_agent = MockLLMAgent(repo_path)
            
            # Agent loop: iterate until tests are fixed or limits reached
            console.print("\n[bold]Starting agent loop...[/bold]")
            
            while state.increment_iteration():
                iteration = state.current_iteration
                console.print(f"\n[blue]‚îÅ‚îÅ‚îÅ Iteration {iteration}/{state.max_iterations} ‚îÅ‚îÅ‚îÅ[/blue]")
                
                # 1. PLANNER: Generate a plan based on failing tests
                console.print(f"[cyan]üß† Planning fix for {state.total_failures} failing test(s)...[/cyan]")
                
                # Log planner start
                telemetry.log_event("planner_start", {
                    "iteration": iteration,
                    "failing_tests": state.total_failures
                })
                
                # Use LLM to create plan (with critic feedback if available)
                critic_feedback = getattr(state, 'critic_feedback', None) if iteration > 1 else None
                plan = llm_agent.create_plan(state.failing_tests, iteration, critic_feedback)
                
                # Store plan in state for reference
                state.plan = plan
                
                # Display plan summary
                if verbose:
                    console.print("[dim]Plan created:[/dim]")
                    if plan.get("approach"):
                        console.print(f"  Approach: {plan['approach']}")
                    if plan.get("steps"):
                        console.print("  Steps:")
                        for i, step in enumerate(plan['steps'], 1):  # Show all steps
                            console.print(f"    {i}. {step}")
                    elif plan.get("strategy"):
                        console.print(f"  Strategy: {plan['strategy']}")
                    
                    # Show which source files will be modified
                    if plan.get("source_files"):
                        console.print(f"  Target files: {', '.join(plan['source_files'])}")
                    else:
                        console.print("  [yellow]‚ö† No source files identified[/yellow]")
                
                # Log planner completion
                telemetry.log_event("planner_complete", {
                    "iteration": iteration,
                    "plan": plan,
                    "failing_tests": state.total_failures
                })
                
                # 2. ACTOR: Generate a patch diff based on the plan
                console.print(f"[cyan]üé≠ Generating patch based on plan...[/cyan]")
                
                # Log actor start
                telemetry.log_event("actor_start", {"iteration": iteration})
                
                # Generate patch with plan context and critic feedback if available
                patch_diff = llm_agent.generate_patch(state.failing_tests, iteration, plan=state.plan, critic_feedback=critic_feedback, state=state)
                
                if not patch_diff:
                    console.print("[red]‚ùå Could not generate a patch[/red]")
                    state.final_status = "no_patch"
                    telemetry.log_event("actor_failed", {"iteration": iteration})
                    break
                
                # Display patch info
                patch_lines = patch_diff.split('\n')
                if verbose:
                    console.print(f"[dim]Generated patch: {len(patch_lines)} lines[/dim]")
                    
                    # Show the actual patch content in verbose mode
                    if whole_file:
                        console.print("\n[bold cyan]File replacements:[/bold cyan]")
                        for line in patch_lines[:50]:  # Show first 50 lines
                            if line.startswith('FILE_REPLACE:'):
                                console.print(f"[bold yellow]{line}[/bold yellow]")
                            elif line == 'END_FILE_REPLACE':
                                console.print(f"[bold yellow]{line}[/bold yellow]\n")
                            else:
                                console.print(f"[dim]{line}[/dim]")
                        if len(patch_lines) > 50:
                            console.print(f"[dim]... ({len(patch_lines) - 50} more lines)[/dim]")
                    else:
                        console.print("\n[bold cyan]Patch preview:[/bold cyan]")
                        for line in patch_lines[:30]:  # Show first 30 lines
                            if line.startswith('+++') or line.startswith('---'):
                                console.print(f"[bold]{line}[/bold]")
                            elif line.startswith('+'):
                                console.print(f"[green]{line}[/green]")
                            elif line.startswith('-'):
                                console.print(f"[red]{line}[/red]")
                            else:
                                console.print(f"[dim]{line}[/dim]")
                        if len(patch_lines) > 30:
                            console.print(f"[dim]... ({len(patch_lines) - 30} more lines)[/dim]")
                
                # Log actor completion
                telemetry.log_event("actor_complete", {
                    "iteration": iteration,
                    "patch_size": len(patch_lines)
                })
                # Save patch artifact (before apply, so we have it even if apply fails)
                telemetry.save_patch(iteration, patch_diff)
                
                # 3. CRITIC: Review and approve/reject the patch
                console.print(f"[cyan]üîç Reviewing patch with critic...[/cyan]")
                
                # Log critic start
                telemetry.log_event("critic_start", {"iteration": iteration})
                
                # If test_after_review is enabled, apply patch temporarily and run tests
                test_results_for_critic = None
                if test_after_review:
                    console.print(f"[dim]Applying patch temporarily to test before review...[/dim]")
                    
                    # Create a temporary branch for testing
                    temp_branch = f"nova-test-{iteration}-{int(time.time())}"
                    original_branch = git_manager._get_current_branch()
                    
                    try:
                        # Create and checkout temp branch
                        git_manager._run_git_command("checkout", "-b", temp_branch)
                        
                        # Apply the patch
                        temp_result = apply_patch(state, patch_diff, git_manager, verbose=False)
                        
                        if temp_result["success"]:
                            # Run tests to get results
                            console.print(f"[dim]Running tests on patched code...[/dim]")
                            test_failures, _ = runner.run_tests()
                            
                            # Capture test results as a string for the critic
                            if test_failures:
                                test_results_for_critic = f"Tests still failing after patch:\n"
                                for failure in test_failures[:5]:  # Show first 5 failures
                                    test_results_for_critic += f"- {failure.test_name}: {failure.error}\n"
                                if len(test_failures) > 5:
                                    test_results_for_critic += f"... and {len(test_failures) - 5} more failures"
                            else:
                                test_results_for_critic = "All tests pass after applying this patch!"
                                
                            console.print(f"[dim]Test results: {len(test_failures)} failures[/dim]")
                        else:
                            test_results_for_critic = f"Patch failed to apply: {temp_result.get('error', 'unknown error')}"
                            
                    except Exception as e:
                        test_results_for_critic = f"Error testing patch: {str(e)}"
                        if verbose:
                            console.print(f"[yellow]Warning: Could not test patch: {e}[/yellow]")
                    finally:
                        # Always go back to original branch and clean up
                        try:
                            git_manager._run_git_command("checkout", original_branch)
                            git_manager._run_git_command("branch", "-D", temp_branch)
                        except:
                            pass
                
                # Use LLM to review patch (with test results if available)
                patch_approved, review_reason = llm_agent.review_patch(
                    patch_diff, 
                    state.failing_tests, 
                    state,
                    test_results_for_critic
                )
                
                if verbose:
                    console.print(f"[dim]Critic review: {review_reason}[/dim]")
                
                if not patch_approved:
                    console.print(f"[red]‚ùå Patch rejected: {review_reason}[/red]")
                    # Store critic feedback for next iteration
                    state.critic_feedback = review_reason
                    telemetry.log_event("critic_rejected", {
                        "iteration": iteration,
                        "reason": review_reason
                    })
                    
                    # Check if we have more iterations available
                    if iteration < state.max_iterations:
                        console.print(f"[yellow]Will try a different approach in iteration {iteration + 1}...[/yellow]")
                        continue  # Try again with critic feedback
                    else:
                        # Only set final status if we're out of iterations
                        state.final_status = "patch_rejected"
                        break
                
                console.print("[green]‚úì Patch approved by critic[/green]")
                
                # Clear critic feedback since patch was approved
                state.critic_feedback = None
                
                # Log critic approval
                telemetry.log_event("critic_approved", {
                    "iteration": iteration,
                    "reason": review_reason
                })
                
                # 4. APPLY PATCH: Apply the approved patch and commit
                console.print(f"[cyan]üìù Applying patch...[/cyan]")
                
                # Use our ApplyPatchNode to apply and commit the patch
                result = apply_patch(state, patch_diff, git_manager, verbose=verbose)
                
                if not result["success"]:
                    console.print(f"[red]‚ùå Failed to apply patch: {result.get('error', 'unknown error')}[/red]")
                    # Provide feedback for next iteration
                    state.critic_feedback = "Patch failed to apply ‚Äì likely incorrect context."
                    telemetry.log_event("patch_error", {
                        "iteration": iteration,
                        "step": result.get("step_number", 0),
                        "error": result.get('error', 'unknown')
                    })
                    if iteration < state.max_iterations:
                        console.print(f"[yellow]‚Üª Retrying with a new patch in iteration {iteration+1}...[/yellow]")
                        continue  # go to next iteration without breaking
                    else:
                        state.final_status = "patch_error"
                        break
                else:
                    # Log successful patch application (only if not already done by fallback)
                    console.print(f"[green]‚úì Patch applied and committed (step {result['step_number']})[/green]")
                telemetry.log_event("patch_applied", {
                    "iteration": iteration,
                    "step": result["step_number"],
                    "files_changed": result["changed_files"],
                    "commit": git_manager._get_current_head()
                })
                
                # Save patch artifact for auditing
                # The patch was already saved before apply, no need to save again
                
                # 5. RUN TESTS: Check if the patch fixed the failures
                console.print(f"[cyan]üß™ Running tests after patch...[/cyan]")
                new_failures, junit_xml = runner.run_tests()
                
                # Save test report artifact
                if junit_xml:
                    telemetry.save_test_report(result['step_number'], junit_xml, report_type="junit")
                    # Update the junit report file if specified
                    if junit_report:
                        try:
                            junit_report.write_text(junit_xml)
                        except Exception:
                            pass  # Silently ignore errors in the loop
                
                # Update state with new test results
                previous_failures = state.total_failures
                state.add_failing_tests(new_failures)
                state.test_results.append({
                    "iteration": iteration,
                    "failures_before": previous_failures,
                    "failures_after": state.total_failures
                })
                
                telemetry.log_event("test_results", {
                    "iteration": iteration,
                    "failures_before": previous_failures,
                    "failures_after": state.total_failures,
                    "fixed": previous_failures - state.total_failures
                })
                
                # 6. REFLECT: Check if we should continue or stop
                telemetry.log_event("reflect_start", {
                    "iteration": iteration,
                    "failures_before": previous_failures,
                    "failures_after": state.total_failures
                })
                
                if state.total_failures == 0:
                    # All tests passed - success!
                    console.print(f"\n[bold green]‚úÖ All tests passing! Fixed in {iteration} iteration(s).[/bold green]")
                    state.final_status = "success"
                    success = True
                    telemetry.log_event("reflect_complete", {
                        "iteration": iteration,
                        "decision": "success",
                        "reason": "all_tests_passing"
                    })
                    break
                
                # Check if we made progress
                if state.total_failures < previous_failures:
                    fixed_count = previous_failures - state.total_failures
                    console.print(f"[green]‚úì Progress: Fixed {fixed_count} test(s), {state.total_failures} remaining[/green]")
                else:
                    console.print(f"[yellow]‚ö† No progress: {state.total_failures} test(s) still failing[/yellow]")
                    # Let the planner/critic know that the last patch had no effect
                    state.critic_feedback = "No progress in reducing failures ‚Äì try a different approach."
                
                # Check timeout
                if state.check_timeout():
                    console.print(f"[red]‚è∞ Timeout reached ({state.timeout_seconds}s)[/red]")
                    state.final_status = "timeout"
                    telemetry.log_event("reflect_complete", {
                        "iteration": iteration,
                        "decision": "stop",
                        "reason": "timeout"
                    })
                    break
                
                # Check if we're at max iterations
                if iteration >= state.max_iterations:
                    console.print(f"[red]üîÑ Maximum iterations reached ({state.max_iterations})[/red]")
                    state.final_status = "max_iters"
                    telemetry.log_event("reflect_complete", {
                        "iteration": iteration,
                        "decision": "stop",
                        "reason": "max_iterations"
                    })
                    break
                
                # Continue to next iteration
                console.print(f"[dim]Continuing to iteration {iteration + 1}...[/dim]")
                telemetry.log_event("reflect_complete", {
                    "iteration": iteration,
                    "decision": "continue",
                    "reason": "more_failures_to_fix"
                })
            
            # Print exit summary
            if state and state.final_status:
                print_exit_summary(state, state.final_status, verbose=verbose)
            
            # Final note about JUnit report
            if junit_report and junit_report.exists():
                console.print(f"\n[dim]JUnit test report saved to: {junit_report}[/dim]")
            
            # Log final completion status
            telemetry.log_event("completion", {
                "status": state.final_status,
                "iterations": state.current_iteration,
                "total_patches": len(state.patches_applied),
                "final_failures": state.total_failures
            })
            telemetry.end_run(success=success)
        
    except KeyboardInterrupt:
        if state:
            state.final_status = "interrupted"
            print_exit_summary(state, "interrupted", verbose=verbose)
        else:
            console.print("\n[yellow]Interrupted by user[/yellow]")
        if telemetry:
            telemetry.log_event("interrupted", {"reason": "keyboard_interrupt"})
        success = False
    except Exception as e:
        console.print(f"\n[red]Error: {e}[/red]")
        if state:
            state.final_status = "error"
            print_exit_summary(state, "error", verbose=verbose)
        if telemetry:
            telemetry.log_event("error", {"error": str(e)})
        success = False
    finally:
        # If successful, offer to create a PR
        pr_created = False
        # Only create PR if we actually had failing tests to fix and made changes
        has_initial_failures = state and hasattr(state, 'initial_failing_tests') and state.initial_failing_tests
        if success and state and branch_name and git_manager and has_initial_failures:
            try:
                console.print("\n[bold green]‚úÖ Success! Changes saved to branch:[/bold green] " + branch_name)
                
                # Optionally skip PR creation via env (e.g., NOVA_SKIP_PR=1)
                if os.environ.get("NOVA_SKIP_PR", "0") == "1":
                    console.print("[dim]Skipping PR creation (NOVA_SKIP_PR=1).[/dim]")
                    pr_created = False
                    return
                
                # In CI mode, we don't create new PRs
                if ci_mode:
                    console.print("\n[green]‚ú® CI Mode: Fixes committed successfully![/green]")
                    console.print("[dim]‚Ä¢ Direct commits to PR branch[/dim]")
                    console.print("[dim]‚Ä¢ GitHub Actions will update PR status[/dim]")
                    
                    # Generate beautiful diff summary
                    if state and state.patches_applied:
                        _generate_ci_diff_summary(state, repo_path)
                    
                    # Post completion comment
                    if not quiet:
                        _post_ci_completion_comment(repo_path, state, verbose)
                    
                    return
                
                # Ask if user wants to create a PR
                from nova.tools.pr_generator import PRGenerator
                pr_gen = PRGenerator(repo_path, verbose=not quiet, truncated=truncated)
                
                # Check if PR already exists
                if pr_gen.check_pr_exists(branch_name):
                    console.print("[yellow]A PR already exists for this branch[/yellow]")
                else:
                    console.print("\n[cyan]ü§ñ Using GPT-5 to generate a pull request...[/cyan]")
                    
                    # Calculate execution time
                    if hasattr(state, 'start_time'):
                        if isinstance(state.start_time, float):
                            elapsed_time = time.time() - state.start_time
                        else:
                            elapsed_time = seconds_between(now_utc(), state.start_time)
                    else:
                        elapsed_time = 0
                    minutes, seconds = divmod(int(elapsed_time), 60)
                    execution_time = f"{minutes}m {seconds}s"
                    
                    # Get fixed tests and changed files
                    # Use initial_failing_tests for PR generation (what we originally fixed)
                    fixed_tests = state.initial_failing_tests if state.initial_failing_tests else []
                    changed_files = []
                    
                    # Get list of changed files from git
                    num_patches = len(state.patches_applied)
                    if num_patches > 0:
                        try:
                            base_ref = f"HEAD~{num_patches}"
                            result = subprocess.run(
                                ["git", "diff", "--name-only", base_ref],
                                capture_output=True,
                                text=True,
                                cwd=repo_path
                            )
                            if result.returncode == 0:
                                changed_files = [f for f in result.stdout.strip().split('\n') if f]
                        except Exception as e:
                            if verbose:
                                console.print(f"[yellow]‚ö†Ô∏è  Could not list changed files: {e}[/yellow]")
                    
                    # Gather reasoning logs from telemetry
                    reasoning_logs = []
                    if telemetry and hasattr(telemetry, 'events'):
                        reasoning_logs = telemetry.events
                    elif telemetry:
                        # Try to read from telemetry files
                        try:
                            telemetry_dir = Path(repo_path) / ".nova"
                            if telemetry_dir.exists():
                                # Get the most recent run directory
                                run_dirs = sorted([d for d in telemetry_dir.iterdir() if d.is_dir()], 
                                                key=lambda x: x.stat().st_mtime, reverse=True)
                                if run_dirs:
                                    trace_file = run_dirs[0] / "trace.jsonl"
                                    if trace_file.exists():
                                        import json
                                        with open(trace_file) as f:
                                            for line in f:
                                                try:
                                                    reasoning_logs.append(json.loads(line))
                                                except json.JSONDecodeError as e:
                                                    # Skip malformed JSON lines
                                                    pass
                        except Exception as e:
                            print(f"[yellow]Could not read reasoning logs: {e}[/yellow]")
                    
                    # Generate PR content using GPT-5
                    console.print("[dim]Generating PR title and description...[/dim]")
                    title, description = pr_gen.generate_pr_content(
                        fixed_tests=fixed_tests,
                        patches_applied=state.patches_applied,
                        changed_files=changed_files,
                        execution_time=execution_time,
                        reasoning_logs=reasoning_logs
                    )
                    
                    # PR content generated but not printed to reduce output
                    if verbose:
                        console.print(f"\n[dim]PR Title: {title}[/dim]")
                    
                    # Push the branch first
                    console.print("\n[cyan]Pushing branch to remote...[/cyan]")
                    push_result = subprocess.run(
                        ["git", "push", "origin", branch_name],
                        capture_output=True,
                        text=True,
                        cwd=repo_path
                    )
                    
                    if push_result.returncode != 0:
                        console.print(f"[yellow]Warning: Failed to push branch: {push_result.stderr}[/yellow]")
                        console.print("[dim]Attempting to create PR anyway...[/dim]")
                    
                    # Create the PR
                    console.print("\n[cyan]Creating pull request...[/cyan]")
                    # Detect the default branch
                    base_branch = git_manager.get_default_branch()
                    success_pr, pr_url_or_error = pr_gen.create_pr(
                        branch_name=branch_name,
                        title=title,
                        description=description,
                        base_branch=base_branch,
                        draft=False
                    )
                    
                    if success_pr:
                        console.print(f"\n[bold green]üéâ Pull Request created successfully![/bold green]")
                        console.print(f"[link={pr_url_or_error}]{pr_url_or_error}[/link]")
                        pr_created = True
                    else:
                        console.print(f"\n[yellow]Could not create PR: {pr_url_or_error}[/yellow]")
                        console.print(f"[dim]You can manually create a PR from branch: {branch_name}[/dim]")
                        
            except Exception as e:
                console.print(f"\n[yellow]Error creating PR: {e}[/yellow]")
                console.print(f"[dim]You can manually create a PR from branch: {branch_name}[/dim]")
        
        # Clean up branch and restore original state (unless PR was created)
        if git_manager and branch_name:
            if pr_created:
                # Don't delete the branch if we created a PR
                git_manager.cleanup(success=True)  # Preserve branch if PR was created
                console.print(f"\n[dim]Branch '{branch_name}' preserved for PR[/dim]")
            else:
                git_manager.cleanup(success=success)
            git_manager.restore_signal_handler()
        # Ensure telemetry run is ended if not already done
        if telemetry and not success and (state is None or state.final_status is None):
            telemetry.end_run(success=False)
        # Exit with appropriate code (0 for success, 1 for failure)
        raise SystemExit(0 if success else 1)


@app.command()
def version():
    """
    Show Nova CI-Rescue version.
    """
    from nova import __version__
    console.print(f"[green]Nova CI-Rescue[/green] v{__version__}")


if __name__ == "__main__":
    app()
