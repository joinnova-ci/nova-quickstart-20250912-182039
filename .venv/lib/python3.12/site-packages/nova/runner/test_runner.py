"""
Test runner module for capturing pytest failures.
"""

import json
import subprocess
import tempfile
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
from rich.console import Console
from nova.config import get_settings

console = Console()


@dataclass
class FailingTest:
    """Represents a failing test with its details."""
    name: str
    file: str
    line: int
    short_traceback: str
    full_traceback: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "file": self.file,
            "line": self.line,
            "short_traceback": self.short_traceback,
        }


class TestRunner:
    """Runs pytest and captures failing tests."""
    
    def __init__(self, repo_path: Path, verbose: bool = False, custom_cmd: Optional[str] = None, junit_report_path: Optional[str] = None):
        self.repo_path = repo_path
        self.verbose = verbose
        self.custom_cmd = custom_cmd
        self.junit_report_path = junit_report_path
        
    def run_tests(self) -> Tuple[List[FailingTest], Optional[str]]:
        """
        Run pytest and capture all failing tests.
            
        Returns:
            Tuple of (List of FailingTest objects, JUnit XML report content)
        """
        console.print("[cyan]Running pytest to identify failing tests...[/cyan]")
        
        # Create temporary files for reports
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as tmp:
            json_report_path = tmp.name
        
        # Use provided junit report path or create a temp file
        if self.junit_report_path:
            junit_report_path = self.junit_report_path
            delete_junit = False
        else:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.xml', delete=False) as tmp:
                junit_report_path = tmp.name
            delete_junit = True
        
        junit_xml_content = None
        
        try:
            if self.custom_cmd:
                # Use custom command - we need to ensure it outputs JSON and JUnit reports
                import shlex
                cmd = shlex.split(self.custom_cmd)
                # Add JSON and JUnit report flags if not already present
                if not any("json-report" in arg for arg in cmd):
                    cmd.extend(["--json-report", f"--json-report-file={json_report_path}"])
                if not any("junit" in arg for arg in cmd):
                    cmd.extend([f"--junit-xml={junit_report_path}"])
                # Add current directory if no path specified
                if not any(arg.startswith(".") or arg.startswith("/") or "test" in arg for arg in cmd[1:]):
                    cmd.append(".")
            else:
                # Run pytest with JSON and JUnit reports
                # Explicitly specify current directory to ensure recursive discovery
                cmd = [
                    "python", "-m", "pytest",
                    ".",  # Current directory - ensures recursive test discovery
                    "--json-report",
                    f"--json-report-file={json_report_path}",
                    f"--junit-xml={junit_report_path}",
                    "--tb=short",
                    "-q",  # Quiet mode
                    "--no-header",
                    "--no-summary",
                    "-rN",  # Don't show any summary info
                ]
            
            if self.verbose:
                console.print(f"[dim]Running: {' '.join(cmd)}[/dim]")
            
            # Run pytest (we expect it to fail if there are failing tests)
            settings = get_settings()
            try:
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    cwd=str(self.repo_path),
                    timeout=settings.test_timeout_sec,
                )
            except FileNotFoundError:
                # Fallback: try invoking pytest via the current Python interpreter
                try:
                    import sys as _sys
                    if self.custom_cmd:
                        # Replace the first token (pytest executable) with 'python -m pytest'
                        # Keep the remaining args (including -q/-xvs and test selectors)
                        args_tail = cmd[1:] if len(cmd) > 1 else []
                        fallback_cmd = [_sys.executable, "-m", "pytest", *args_tail]
                    else:
                        fallback_cmd = [_sys.executable, "-m", "pytest", ".",
                                         "--json-report", f"--json-report-file={json_report_path}",
                                         f"--junit-xml={junit_report_path}", "--tb=short", "-q",
                                         "--no-header", "--no-summary", "-rN"]
                    result = subprocess.run(
                        fallback_cmd,
                        capture_output=True,
                        text=True,
                        cwd=str(self.repo_path),
                        timeout=settings.test_timeout_sec,
                    )
                except FileNotFoundError:
                    console.print("[red]Error: pytest not found. Please install pytest.[/red]")
                    return [], None
            
            # Parse the JSON report
            failing_tests = self._parse_json_report(json_report_path)
            
            # Read the JUnit XML report if it exists
            junit_path = Path(junit_report_path)
            if junit_path.exists():
                junit_xml_content = junit_path.read_text()
            
            # Fallback: if JSON is empty but JUnit shows failures, parse JUnit
            if not failing_tests and junit_xml_content and "<failure" in junit_xml_content:
                try:
                    failing_tests = self._parse_junit_xml(junit_xml_content)
                except Exception:
                    # Ignore parsing errors; will fall through to generic handling
                    pass
            
            if not failing_tests:
                console.print("[green]âœ“ No failing tests found![/green]")
                return [], junit_xml_content
            
            console.print(f"[yellow]Found {len(failing_tests)} failing test(s)[/yellow]")
            return failing_tests, junit_xml_content
            
        except FileNotFoundError:
            # pytest not installed or not found
            console.print("[red]Error: pytest not found. Please install pytest.[/red]")
            return [], None
        except Exception as e:
            console.print(f"[red]Error running tests: {e}[/red]")
            return [], None
        finally:
            # Clean up temp files
            Path(json_report_path).unlink(missing_ok=True)
            if delete_junit:
                Path(junit_report_path).unlink(missing_ok=True)

    def _parse_junit_xml(self, xml_content: str) -> List[FailingTest]:
        """Parse JUnit XML content to extract failing tests as a fallback.
        Attempts to infer file paths from 'file' or 'classname' attributes.
        """
        import xml.etree.ElementTree as ET
        failing: List[FailingTest] = []
        try:
            root = ET.fromstring(xml_content)
        except ET.ParseError:
            return failing
        # Handle both <testsuite> root or <testsuites>
        testcases = []
        if root.tag.endswith('testsuite'):
            testcases = root.findall('.//testcase')
        elif root.tag.endswith('testsuites'):
            testcases = root.findall('.//testcase')
        for tc in testcases:
            failure = tc.find('failure') or tc.find('error')
            if failure is None:
                continue
            name = tc.get('name') or 'unknown'
            classname = tc.get('classname') or ''
            file_attr = tc.get('file') or ''
            file_path = file_attr
            if not file_path:
                # Derive from classname when possible (module style path)
                if classname:
                    # Best-effort: last component is module
                    module = classname.split('.')[-1]
                    if module:
                        file_path = f"{module}.py"
            # Short traceback from failure text or message
            text = (failure.text or failure.get('message') or 'Test failed').strip()
            short_text = text.split('\n', 1)[0][:200]
            failing.append(FailingTest(
                name=name,
                file=file_path or 'unknown',
                line=0,
                short_traceback=short_text or 'Test failed',
            ))
        return failing
    
    def _parse_junit_xml(self, xml_content: str) -> List[FailingTest]:
        """Parse JUnit XML content to extract failing tests as a fallback.
        Attempts to infer file paths from 'file' or 'classname' attributes.
        """
        import xml.etree.ElementTree as ET
        failing: List[FailingTest] = []
        try:
            root = ET.fromstring(xml_content)
        except ET.ParseError:
            return failing
        
        # Handle both <testsuite> root or <testsuites>
        testcases = []
        if root.tag.endswith('testsuite'):
            testcases = root.findall('.//testcase')
        elif root.tag.endswith('testsuites'):
            testcases = root.findall('.//testcase')
        
        for tc in testcases:
            failure = tc.find('failure') or tc.find('error')
            if failure is None:
                continue
            
            name = tc.get('name') or 'unknown'
            classname = tc.get('classname') or ''
            file_attr = tc.get('file') or ''
            file_path = file_attr
            
            if not file_path:
                # Derive from classname when possible (module style path)
                if classname:
                    # Best-effort: last component is module
                    module = classname.split('.')[-1]
                    if module:
                        file_path = f"{module}.py"
            
            # Short traceback from failure text or message
            text = (failure.text or failure.get('message') or 'Test failed').strip()
            short_text = text.split('\n', 1)[0][:200]
            
            failing.append(FailingTest(
                name=name,
                file=file_path or 'unknown',
                line=0,
                short_traceback=short_text or 'Test failed',
            ))
        
        return failing
    
    def _parse_json_report(self, report_path: str) -> List[FailingTest]:
        """Parse pytest JSON report to extract failing tests."""
        try:
            with open(report_path, 'r') as f:
                report = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            return []
        
        failing_tests = []
        
        # Extract failing tests from the report
        for test in report.get('tests', []):
            if test.get('outcome') in ['failed', 'error']:
                # Extract test details
                nodeid = test.get('nodeid', '')
                
                # Parse file and line from nodeid (format: path/to/test.py::TestClass::test_method)
                if '::' in nodeid:
                    file_part, test_part = nodeid.split('::', 1)
                    test_name = test_part.replace('::', '.')
                else:
                    file_part = nodeid
                    test_name = Path(nodeid).stem
                
                # Normalize the file path to be relative to repo root
                # Remove repo directory prefix if it's included in the path
                repo_name = self.repo_path.name
                if file_part.startswith(f"{repo_name}/"):
                    file_part = file_part[len(repo_name)+1:]
                
                # Get the traceback
                call_info = test.get('call', {})
                longrepr = call_info.get('longrepr', '')
                
                # Extract short traceback - capture up to the assertion error line
                traceback_lines = longrepr.split('\n') if longrepr else []
                short_trace = []
                for line in traceback_lines:
                    short_trace.append(line)
                    if line.strip().startswith("E"):  # error/exception line
                        break
                    if len(short_trace) >= 5:
                        break
                short_traceback = '\n'.join(short_trace) if short_trace else 'Test failed'
                
                # Try to get line number from the traceback
                line_no = 0
                for line in traceback_lines:
                    if file_part in line and ':' in line:
                        try:
                            # Extract line number from traceback line like "test.py:42"
                            parts = line.split(':')
                            for i, part in enumerate(parts):
                                if file_part in part and i + 1 < len(parts):
                                    line_no = int(parts[i + 1].split()[0])
                                    break
                        except (ValueError, IndexError):
                            pass
                
                failing_tests.append(FailingTest(
                    name=test_name,
                    file=file_part,
                    line=line_no,
                    short_traceback=short_traceback,
                    full_traceback=longrepr,
                ))
        
        return failing_tests
    
    def format_failures_table(self, failures: List[FailingTest]) -> str:
        """Format failing tests as a markdown table for the planner prompt."""
        if not failures:
            return "No failing tests found."
        
        table = "| Test Name | File:Line | Error |\n"
        table += "|-----------|-----------|-------|\n"
        
        for test in failures:
            location = f"{test.file}:{test.line}" if test.line > 0 else test.file
            # Extract the most relevant error line (assertion or exception)
            error_lines = test.short_traceback.split('\n')
            error = "Test failed"
            for line in error_lines:
                if line.strip().startswith("E"):
                    error = line.strip()[2:].strip()  # Remove "E " prefix
                    break
                elif "AssertionError" in line or "assert" in line:
                    error = line.strip()
                    break
            # Truncate if too long
            if len(error) > 80:
                error = error[:77] + "..."
            table += f"| {test.name} | {location} | {error} |\n"
        
        return table
